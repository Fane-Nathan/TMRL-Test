# standard library imports
import csv
import json
import os
import time
from dataclasses import dataclass
from pathlib import Path

# third-party imports
import torch
from pandas import DataFrame

# local imports
import tmrl.config.config_constants as cfg
from tmrl.util import pandas_dict

import logging


__docformat__ = "google"


def _ma_last(values, window=10):
    if not values:
        return 0.0
    tail = values[-window:]
    return float(sum(tail) / len(tail))


def compute_stall_state(eval_returns,
                        train_returns,
                        best_eval_ma10=None,
                        stall_epochs=0,
                        improvement_threshold=0.05,
                        patience=10):
    """
    Computes rolling anti-stall diagnostics from epoch-level eval/train returns.

    Args:
        eval_returns (List[float]): epoch-level evaluation returns.
        train_returns (List[float]): epoch-level training returns.
        best_eval_ma10 (float or None): best rolling eval MA10 observed so far.
        stall_epochs (int): number of consecutive epochs without meaningful eval MA10 improvement.
        improvement_threshold (float): relative improvement threshold (e.g. 0.05 = 5%).
        patience (int): epochs without improvement before flagged as stalled.
    """
    eval_hist = []
    train_hist = []
    eval_ma10 = 0.0
    train_ma10 = 0.0
    best = best_eval_ma10
    stall = int(stall_epochs)

    for eval_ret, train_ret in zip(eval_returns, train_returns):
        eval_hist.append(float(eval_ret))
        train_hist.append(float(train_ret))
        eval_ma10 = _ma_last(eval_hist, window=10)
        train_ma10 = _ma_last(train_hist, window=10)

        if best is None:
            best = eval_ma10
            stall = 0
            continue

        if best > 0.0:
            improved = eval_ma10 >= best * (1.0 + improvement_threshold)
        else:
            improved = eval_ma10 > best + 1e-9

        if improved:
            best = eval_ma10
            stall = 0
        else:
            stall += 1

    return {
        "eval_return_ma10": float(eval_ma10),
        "train_return_ma10": float(train_ma10),
        "best_eval_ma10": float(best if best is not None else 0.0),
        "stall_epochs": int(stall),
        "stalled": bool(stall >= patience),
        "improvement_threshold": float(improvement_threshold),
        "patience": int(patience),
    }


def compute_stall_dashboard(stall_state, warning_epochs=5):
    """
    Returns a compact dashboard state from stall metrics.
    """
    stall_epochs = int(stall_state.get("stall_epochs", 0))
    patience = int(stall_state.get("patience", 10))
    if stall_epochs >= patience:
        status = "stalled"
    elif stall_epochs >= warning_epochs:
        status = "warning"
    else:
        status = "healthy"
    return {
        "status": status,
        "warning_epochs": int(warning_epochs),
    }


@dataclass(eq=0)
class TrainingOffline:
    """
    Training wrapper for off-policy algorithms.

    Args:
        env_cls (type): class of a dummy environment, used only to retrieve observation and action spaces if needed. Alternatively, this can be a tuple of the form (observation_space, action_space).
        memory_cls (type): class of the replay memory
        training_agent_cls (type): class of the training agent
        epochs (int): total number of epochs, we save the agent every epoch
        rounds (int): number of rounds per epoch, we generate statistics every round
        steps (int): number of training steps per round
        update_model_interval (int): number of training steps between model broadcasts
        update_buffer_interval (int): number of training steps between retrieving buffered samples
        max_training_steps_per_env_step (float): training will pause when above this ratio
        sleep_between_buffer_retrieval_attempts (float): algorithm will sleep for this amount of time when waiting for needed incoming samples
        profiling (bool): if True, run_epoch will be profiled and the profiling will be printed at the end of each epoch
        agent_scheduler (callable): if not None, must be of the form f(Agent, epoch), called at the beginning of each epoch
        start_training (int): minimum number of samples in the replay buffer before starting training
        device (str): device on which the memory will collate training samples
    """
    env_cls: type = None  # = GenericGymEnv  # dummy environment, used only to retrieve observation and action spaces if needed
    memory_cls: type = None  # = TorchMemory  # replay memory
    training_agent_cls: type = None  # = TrainingAgent  # training agent
    epochs: int = 10  # total number of epochs, we save the agent every epoch
    rounds: int = 50  # number of rounds per epoch, we generate statistics every round
    steps: int = 2000  # number of training steps per round
    update_model_interval: int = 100  # number of training steps between model broadcasts
    update_buffer_interval: int = 100  # number of training steps between retrieving buffered samples
    max_training_steps_per_env_step: float = 1.0  # training will pause when above this ratio
    sleep_between_buffer_retrieval_attempts: float = 1.0  # algorithm will sleep for this amount of time when waiting for needed incoming samples
    profiling: bool = False  # if True, run_epoch will be profiled and the profiling will be printed at the end of each epoch
    agent_scheduler: callable = None  # if not None, must be of the form f(Agent, epoch), called at the beginning of each epoch
    start_training: int = 0  # minimum number of samples in the replay buffer before starting training
    device: str = None  # device on which the model of the TrainingAgent will live

    total_updates = 0

    def __post_init__(self):
        device = self.device
        self.epoch = 0
        self.memory = self.memory_cls(nb_steps=self.steps, device=device)
        if type(self.env_cls) == tuple:
            observation_space, action_space = self.env_cls
        else:
            with self.env_cls() as env:
                observation_space, action_space = env.observation_space, env.action_space
        self.agent = self.training_agent_cls(observation_space=observation_space,
                                             action_space=action_space,
                                             device=device)
        self.total_samples = len(self.memory)
        logging.info(f" Initial total_samples:{self.total_samples}")

        # === CSV Logger for Ablation Study ===
        self._init_csv_logger()
        self._ensure_stall_tracking_state()

    def _resolve_run_name(self):
        """Resolve run name from env override, then config.json, then fallback."""
        env_name = os.environ.get("TMRL_RUN_NAME")
        if env_name:
            return env_name

        config_file = Path.home() / "TmrlData" / "config" / "config.json"
        if config_file.exists():
            try:
                import json
                with open(config_file, encoding="utf-8") as f:
                    config = json.load(f)
                cfg_name = config.get("RUN_NAME")
                if cfg_name:
                    return cfg_name
            except Exception:
                pass
        return "default"

    def _init_csv_logger(self):
        """Initialize CSV logger (called from __post_init__ and __setstate__)."""
        self._csv_file = None
        self._csv_writer = None
        self._training_start_time = time.time()
        ablation_dir = Path(os.environ.get("TMRL_ABLATION_DIR", Path.home() / "TmrlData" / "ablation"))
        ablation_dir.mkdir(parents=True, exist_ok=True)
        run_name = self._resolve_run_name()
        csv_path = ablation_dir / f"{run_name}.csv"
        csv_has_content = csv_path.exists() and csv_path.stat().st_size > 0
        self._csv_file = open(csv_path, "a", newline="")
        self._csv_writer = csv.writer(self._csv_file)
        self._csv_header_written = csv_has_content
        self._csv_path = csv_path
        self._stall_diag_path = ablation_dir / f"{run_name}_stall.json"
        logging.info(f" CSV logging to: {csv_path}")

    def _ensure_stall_tracking_state(self):
        # Backward-compatible defaults for older checkpoints.
        if not hasattr(self, "_eval_epoch_returns"):
            self._eval_epoch_returns = []
        if not hasattr(self, "_train_epoch_returns"):
            self._train_epoch_returns = []
        if not hasattr(self, "_best_eval_ma10"):
            self._best_eval_ma10 = None
        if not hasattr(self, "_stall_epochs"):
            self._stall_epochs = 0
        if not hasattr(self, "_stall_improvement_threshold"):
            self._stall_improvement_threshold = 0.05
        if not hasattr(self, "_stall_patience"):
            self._stall_patience = 10
        if not hasattr(self, "_stall_warning_epochs"):
            self._stall_warning_epochs = 5
        if not hasattr(self, "_stall_last_auto_action"):
            self._stall_last_auto_action = "none"
        if not hasattr(self, "_stall_last_action_epoch"):
            self._stall_last_action_epoch = -1

    def _apply_auto_stall_recovery(self, stall_state):
        """
        Applies safe, bounded auto-recovery actions when stalled.
        """
        action_parts = []
        if not stall_state.get("stalled", False):
            self._stall_last_auto_action = "none"
            return "none"

        # Avoid repeated mutations in the same epoch.
        if self._stall_last_action_epoch == self.epoch:
            return self._stall_last_auto_action

        # 1) Reduce UTD pressure when stalled.
        if hasattr(self.agent, "q_updates_per_policy_update"):
            old_utd = int(self.agent.q_updates_per_policy_update)
            new_utd = max(8, old_utd - 2)
            if new_utd != old_utd:
                self.agent.q_updates_per_policy_update = new_utd
                action_parts.append(f"utd:{old_utd}->{new_utd}")

        # 2) Raise entropy floor slightly to recover exploration.
        if hasattr(self.agent, "alpha_floor"):
            old_floor = float(self.agent.alpha_floor)
            new_floor = min(0.12, old_floor + 0.02)
            if abs(new_floor - old_floor) > 1e-12:
                self.agent.alpha_floor = new_floor
                action_parts.append(f"alpha_floor:{old_floor:.3f}->{new_floor:.3f}")

        action = ",".join(action_parts) if action_parts else "hold(stalled,no_change)"
        self._stall_last_auto_action = action
        self._stall_last_action_epoch = self.epoch
        return action

    def _update_stall_diagnostics(self, epoch_stats):
        if not epoch_stats:
            return

        train_epoch_return = float(sum(float(s.get("return_train", 0.0)) for s in epoch_stats) / len(epoch_stats))
        self._train_epoch_returns.append(train_epoch_return)

        # If eval episodes are disabled, don't derive stall state from stale test stats.
        eval_enabled = int(getattr(cfg, "TEST_EPISODE_INTERVAL", 0)) > 0
        if not eval_enabled:
            train_ma10 = _ma_last(self._train_epoch_returns, window=10)
            self._stall_epochs = 0
            self._stall_last_auto_action = "disabled(eval_off)"
            payload = {
                "epoch": int(self.epoch),
                "eval_enabled": False,
                "eval_epoch_return": None,
                "train_epoch_return": train_epoch_return,
                "eval_return_ma10": None,
                "train_return_ma10": train_ma10,
                "best_eval_ma10": None,
                "stalled": False,
                "stall_epochs": 0,
                "improvement_threshold": self._stall_improvement_threshold,
                "patience": self._stall_patience,
                "status": "disabled",
                "auto_action": self._stall_last_auto_action,
                "updated_at_unix": time.time(),
            }
            with open(self._stall_diag_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, indent=2, sort_keys=True)
            logging.info(
                f" Stall diagnostics: eval=disabled (TEST_EPISODE_INTERVAL=0), "
                f"train_return_ma10={train_ma10:.6f}"
            )
            return

        eval_epoch_return = float(sum(float(s.get("return_test", 0.0)) for s in epoch_stats) / len(epoch_stats))
        self._eval_epoch_returns.append(eval_epoch_return)

        state = compute_stall_state(
            eval_returns=self._eval_epoch_returns,
            train_returns=self._train_epoch_returns,
            best_eval_ma10=None,
            stall_epochs=0,
            improvement_threshold=self._stall_improvement_threshold,
            patience=self._stall_patience,
        )
        self._best_eval_ma10 = state["best_eval_ma10"]
        self._stall_epochs = state["stall_epochs"]
        dashboard = compute_stall_dashboard(state, warning_epochs=self._stall_warning_epochs)
        auto_action = self._apply_auto_stall_recovery(state)

        payload = {
            "epoch": int(self.epoch),
            "eval_epoch_return": eval_epoch_return,
            "train_epoch_return": train_epoch_return,
            "eval_return_ma10": state["eval_return_ma10"],
            "train_return_ma10": state["train_return_ma10"],
            "best_eval_ma10": state["best_eval_ma10"],
            "stalled": state["stalled"],
            "stall_epochs": state["stall_epochs"],
            "improvement_threshold": state["improvement_threshold"],
            "patience": state["patience"],
            "status": dashboard["status"],
            "auto_action": auto_action,
            "updated_at_unix": time.time(),
        }

        with open(self._stall_diag_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, indent=2, sort_keys=True)

        logging.info(
            f" Stall diagnostics: eval_return_ma10={state['eval_return_ma10']:.6f}, "
            f"train_return_ma10={state['train_return_ma10']:.6f}, "
            f"stalled={state['stalled']}, stall_epochs={state['stall_epochs']}"
        )
        logging.info(
            f" Anti-stall dashboard: status={dashboard['status']}, "
            f"auto_action={auto_action}, "
            f"eval_ma10={state['eval_return_ma10']:.6f}, "
            f"train_ma10={state['train_return_ma10']:.6f}, "
            f"stall_epochs={state['stall_epochs']}/{state['patience']}"
        )

    def __getstate__(self):
        """Exclude unpicklable CSV file handle from checkpoint."""
        state = self.__dict__.copy()
        state.pop('_csv_file', None)
        state.pop('_csv_writer', None)
        state.pop('_csv_header_written', None)
        state.pop('_training_start_time', None)
        return state

    def __setstate__(self, state):
        """Reinitialize CSV logger after loading checkpoint."""
        self.__dict__.update(state)
        self._init_csv_logger()
        self._ensure_stall_tracking_state()

    def update_buffer(self, interface):
        buffer = interface.retrieve_buffer()
        self.memory.append(buffer)
        self.total_samples += len(buffer)

    def check_ratio(self, interface):
        ratio = self.total_updates / self.total_samples if self.total_samples > 0.0 and self.total_samples >= self.start_training else -1.0
        if ratio > self.max_training_steps_per_env_step or ratio == -1.0:
            logging.info(f" Waiting for new samples")
            while ratio > self.max_training_steps_per_env_step or ratio == -1.0:
                # wait for new samples
                self.update_buffer(interface)
                ratio = self.total_updates / self.total_samples if self.total_samples > 0.0 and self.total_samples >= self.start_training else -1.0
                if ratio > self.max_training_steps_per_env_step or ratio == -1.0:
                    time.sleep(self.sleep_between_buffer_retrieval_attempts)
            logging.info(f" Resuming training")

    def run_epoch(self, interface):
        stats = []
        state = None

        if self.agent_scheduler is not None:
            self.agent_scheduler(self.agent, self.epoch)

        for rnd in range(self.rounds):
            logging.info(f"=== epoch {self.epoch}/{self.epochs} ".ljust(20, '=') + f" round {rnd}/{self.rounds} ".ljust(50, '='))
            logging.debug(f"(Training): current memory size:{len(self.memory)}")

            stats_training = []

            t0 = time.time()
            self.check_ratio(interface)
            t1 = time.time()

            if self.profiling:
                from pyinstrument import Profiler
                pro = Profiler()
                pro.start()

            t2 = time.time()

            t_sample_prev = t2

            for batch in self.memory:  # this samples a fixed number of batches

                t_sample = time.time()

                if self.total_updates % self.update_buffer_interval == 0:
                    # retrieve local buffer in replay memory
                    self.update_buffer(interface)

                t_update_buffer = time.time()

                if self.total_updates == 0:
                    logging.info(f"starting training")

                stats_training_dict = self.agent.train(batch)

                t_train = time.time()
                eval_enabled = int(getattr(cfg, "TEST_EPISODE_INTERVAL", 0)) > 0
                if eval_enabled:
                    stats_training_dict["return_test"] = self.memory.stat_test_return
                    stats_training_dict["episode_length_test"] = self.memory.stat_test_steps
                    stats_training_dict["return_test_det"] = getattr(self.memory, "stat_test_return_det", self.memory.stat_test_return)
                    stats_training_dict["return_test_stoch"] = getattr(self.memory, "stat_test_return_stoch", self.memory.stat_test_return)
                    stats_training_dict["episode_length_test_det"] = getattr(self.memory, "stat_test_steps_det", self.memory.stat_test_steps)
                    stats_training_dict["episode_length_test_stoch"] = getattr(self.memory, "stat_test_steps_stoch", self.memory.stat_test_steps)
                else:
                    stats_training_dict["return_test"] = 0.0
                    stats_training_dict["episode_length_test"] = 0.0
                    stats_training_dict["return_test_det"] = 0.0
                    stats_training_dict["return_test_stoch"] = 0.0
                    stats_training_dict["episode_length_test_det"] = 0.0
                    stats_training_dict["episode_length_test_stoch"] = 0.0
                stats_training_dict["return_train"] = self.memory.stat_train_return
                stats_training_dict["episode_length_train"] = self.memory.stat_train_steps
                stats_training_dict["sampling_duration"] = t_sample - t_sample_prev
                stats_training_dict["training_step_duration"] = t_train - t_update_buffer
                stats_training += stats_training_dict,
                self.total_updates += 1
                if self.total_updates % self.update_model_interval == 0:
                    # broadcast model weights
                    interface.broadcast_model(self.agent.get_actor())
                self.check_ratio(interface)

                t_sample_prev = time.time()

            t3 = time.time()

            round_time = t3 - t0
            idle_time = t1 - t0
            update_buf_time = t2 - t1
            train_time = t3 - t2
            logging.debug(f"round_time:{round_time}, idle_time:{idle_time}, update_buf_time:{update_buf_time}, train_time:{train_time}")
            stats += pandas_dict(memory_len=len(self.memory), round_time=round_time, idle_time=idle_time, **DataFrame(stats_training).mean(skipna=True)),

            logging.info(stats[-1].add_prefix("  ").to_string() + '\n')

            # === CSV: append round metrics ===
            if getattr(self, '_csv_writer', None) is not None:
                row_data = stats[-1]
                if not self._csv_header_written:
                    header = ["wall_clock_seconds", "epoch", "round"] + list(row_data.index)
                    self._csv_writer.writerow(header)
                    self._csv_header_written = True
                elapsed = time.time() - self._training_start_time
                values = [f"{elapsed:.1f}", self.epoch, rnd] + [f"{v:.6f}" if isinstance(v, float) else str(v) for v in row_data.values]
                self._csv_writer.writerow(values)
                self._csv_file.flush()

            if self.profiling:
                pro.stop()
                logging.info(pro.output_text(unicode=True, color=False, show_all=True))

        # Epoch-level anti-stall diagnostics.
        self._update_stall_diagnostics(stats)
        self.epoch += 1
        return stats


class TorchTrainingOffline(TrainingOffline):
    """
    TrainingOffline for trainers based on PyTorch.

    This class implements automatic device selection with PyTorch.
    """
    def __init__(self,
                 env_cls: type = None,
                 memory_cls: type = None,
                 training_agent_cls: type = None,
                 epochs: int = 10,
                 rounds: int = 50,
                 steps: int = 2000,
                 update_model_interval: int = 100,
                 update_buffer_interval: int = 100,
                 max_training_steps_per_env_step: float = 1.0,
                 sleep_between_buffer_retrieval_attempts: float = 1.0,
                 profiling: bool = False,
                 agent_scheduler: callable = None,
                 start_training: int = 0,
                 device: str = None):
        """
        Same arguments as `TrainingOffline`, but when `device` is `None` it is selected automatically for torch.

        Args:
            env_cls (type): class of a dummy environment, used only to retrieve observation and action spaces if needed. Alternatively, this can be a tuple of the form (observation_space, action_space).
            memory_cls (type): class of the replay memory
            training_agent_cls (type): class of the training agent
            epochs (int): total number of epochs, we save the agent every epoch
            rounds (int): number of rounds per epoch, we generate statistics every round
            steps (int): number of training steps per round
            update_model_interval (int): number of training steps between model broadcasts
            update_buffer_interval (int): number of training steps between retrieving buffered samples
            max_training_steps_per_env_step (float): training will pause when above this ratio
            sleep_between_buffer_retrieval_attempts (float): algorithm will sleep for this amount of time when waiting for needed incoming samples
            profiling (bool): if True, run_epoch will be profiled and the profiling will be printed at the end of each epoch
            agent_scheduler (callable): if not None, must be of the form f(Agent, epoch), called at the beginning of each epoch
            start_training (int): minimum number of samples in the replay buffer before starting training
            device (str): device on which the memory will collate training samples (None for automatic)
        """
        device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        super().__init__(env_cls,
                         memory_cls,
                         training_agent_cls,
                         epochs,
                         rounds,
                         steps,
                         update_model_interval,
                         update_buffer_interval,
                         max_training_steps_per_env_step,
                         sleep_between_buffer_retrieval_attempts,
                         profiling,
                         agent_scheduler,
                         start_training,
                         device)
